{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TorchSharp to Generate Synthetic Data for a Regression Problem\n",
    "\n",
    "This tutorial is based on a [PyTorch example](https://jamesmccaffrey.wordpress.com/2023/06/09/using-pytorch-to-generate-synthetic-data-for-a-regression-problem/) posted by James D. McCaffrey on his blog, ported to TorchSharp.\n",
    "\n",
    "Synthetic data sets can be very useful when evaluating and choosing a model.\n",
    "\n",
    "Note that we're taking some shortcuts in this example -- rather than writing the data set as a text file that can be loaded from any modeling framework, we're saving the data as serialized TorchSharp tensors. Is should be straight-forward to modify the tutorial to write the data sets as text, instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: TorchSharp-cpu\"\n",
    "\n",
    "using TorchSharp;\n",
    "using static TorchSharp.TensorExtensionMethods;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Network\n",
    "Neural networks can be used to generate data as well as train. The synthetic data can then be used to evaluate different models to see how well they can copy the behavior of the network used to produce the data.\n",
    "\n",
    "First, we will create the model that will be used to generate the synthetic data. Later, we'll construct a second model that will be trained on the data the first model generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "class Net : torch.nn.Module<torch.Tensor,torch.Tensor>\n",
    "{\n",
    "    private torch.nn.Module<torch.Tensor,torch.Tensor> hid1;\n",
    "    private torch.nn.Module<torch.Tensor,torch.Tensor> oupt;\n",
    "\n",
    "    public Net(int n_in) : base(nameof(Net))\n",
    "    {\n",
    "        var h = torch.nn.Linear(n_in, 10);\n",
    "        var o =  torch.nn.Linear(10,1);\n",
    "\n",
    "        var lim = 0.80;\n",
    "        torch.nn.init.uniform_(h.weight, -lim, lim);\n",
    "        torch.nn.init.uniform_(h.bias, -lim, lim);\n",
    "        torch.nn.init.uniform_(o.weight, -lim, lim);\n",
    "        torch.nn.init.uniform_(o.bias, -lim, lim);\n",
    "\n",
    "        hid1 = h;\n",
    "        oupt = o;\n",
    "\n",
    "        RegisterComponents();\n",
    "    }\n",
    "    public override torch.Tensor forward(torch.Tensor input)\n",
    "    {\n",
    "        using var _ = torch.NewDisposeScope();\n",
    "        var z = hid1.call(input).tanh_();\n",
    "        z = oupt.call(z).sigmoid_();\n",
    "        return z.MoveToOuterDisposeScope();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our generative network, we can define the method to create the data set. If you compare this with the PyTorch code, you will notice that we're relying on TorchSharp to generate a whole batch of data at once, rather than looping. We're also using TorchSharp instead of Numpy for the noise-generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "void CreateDataFile(Net net, int n_in, string fileName, int n_items)\n",
    "{\n",
    "\n",
    "    var x_lo = -1.0;\n",
    "    var x_hi = 1.0;\n",
    "\n",
    "    var X = (x_hi - x_lo) * torch.rand(new long[] {n_items, n_in}) + x_lo;\n",
    "\n",
    "    torch.Tensor y;\n",
    "\n",
    "    using (torch.no_grad()) {\n",
    "        y = net.call(X);\n",
    "    }\n",
    "\n",
    "    // Add some noise in order not to make it too easy to train...\n",
    "    y += torch.randn(y.shape) * 0.01;\n",
    "\n",
    "    // Make sure that the output isn't negative.\n",
    "    y = torch.where(y < 0.0, y + 0.01 * torch.randn(y.shape) + 0.01, y);\n",
    "\n",
    "    // Save the data in two separate, binary files.\n",
    "    X.save(fileName + \".x\");\n",
    "    y.save(fileName + \".y\");\n",
    "}\n",
    "\n",
    "(torch.Tensor X, torch.Tensor y) LoadDataFile(string fileName)\n",
    "{\n",
    "    return (torch.Tensor.load(fileName + \".x\"), torch.Tensor.load(fileName + \".y\"));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var net = new Net(6);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "CreateDataFile(net, 6, \"train.dat\", 2000);\n",
    "CreateDataFile(net, 6, \"test.dat\", 400);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Data\n",
    "\n",
    "Load the data from files again. This is just to demonstrate how to get the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var (X_train, y_train) = LoadDataFile(\"train.dat\");\n",
    "var (X_test, y_test) = LoadDataFile(\"test.dat\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another class, with slightly different logic, and train it on the generated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "class Net2 : torch.nn.Module<torch.Tensor,torch.Tensor>\n",
    "{\n",
    "    private torch.nn.Module<torch.Tensor,torch.Tensor> hid1;\n",
    "    private torch.nn.Module<torch.Tensor,torch.Tensor> oupt;\n",
    "\n",
    "    public Net2(int n_in) : base(nameof(Net2))\n",
    "    {\n",
    "        hid1 = torch.nn.Linear(n_in, 5);\n",
    "        oupt =  torch.nn.Linear(5,1);\n",
    "\n",
    "        RegisterComponents();\n",
    "    }\n",
    "    public override torch.Tensor forward(torch.Tensor input)\n",
    "    {\n",
    "        using var _ = torch.NewDisposeScope();\n",
    "        var z = hid1.call(input).relu_();\n",
    "        z = oupt.call(z).sigmoid_();\n",
    "        return z.MoveToOuterDisposeScope();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the second network, choose a loss to use, and then you're ready to train it. You also need an optimizer and maybe even an LR scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var model = new Net2(6);\n",
    "\n",
    "var loss = torch.nn.MSELoss();\n",
    "\n",
    "var learning_rate = 0.01f;\n",
    "var optimizer = torch.optim.Rprop(model.parameters(), learning_rate);\n",
    "var scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty standard training loop. The input is just in one batch. It ends with evaluating the trained model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(\" initial loss = \" + loss.forward(model.forward(X_train), y_train).item<float>().ToString());\n",
    "\n",
    "for (int i = 0; i < 10000; i++) {\n",
    "\n",
    "    // Compute the loss\n",
    "    using var output = loss.forward(model.forward(X_train), y_train);\n",
    "\n",
    "    // Clear the gradients before doing the back-propagation\n",
    "    model.zero_grad();\n",
    "\n",
    "    // Do back-progatation, which computes all the gradients.\n",
    "    output.backward();\n",
    "\n",
    "    optimizer.step();\n",
    "    \n",
    "    if (i % 100 == 99) {\n",
    "        scheduler.step();\n",
    "    }\n",
    "}\n",
    "\n",
    "Console.WriteLine(\" final loss   = \" + loss.forward(model.forward(X_train), y_train).item<float>());"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we're really curious about is how the second model does on the test set, which it didn't see during training. If the loss is significantly greater than the one from the training set, we need to train more, i.e. start another epoch. If the test set loss doesn't get closer to the training set loss with more epochs, we may need more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "loss.forward(model.forward(X_test), y_test).item<float>()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data into Batches\n",
    "\n",
    "If we want to be a little bit more advanced, we can split the training set into batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var N = X_train.shape[0]/10;\n",
    "var X_batch = X_train.split(N);\n",
    "var y_batch = y_train.split(N);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means modifying the training loop, too. Running multiple batches can take longer, but the model may converge quicker, so the total time before you have the desired model may still be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(\" initial loss = \" + loss.forward(model.forward(X_train), y_train).item<float>().ToString());\n",
    "\n",
    "for (int i = 0; i < 5000; i++) {\n",
    "\n",
    "    for (var j = 0; j < X_batch.Length; j++) {\n",
    "        // Compute the loss\n",
    "        using var output = loss.forward(model.forward(X_batch[j]), y_batch[j]);\n",
    "\n",
    "        // Clear the gradients before doing the back-propagation\n",
    "        model.zero_grad();\n",
    "\n",
    "        // Do back-progatation, which computes all the gradients.\n",
    "        output.backward();\n",
    "\n",
    "        optimizer.step();\n",
    "    }\n",
    "    \n",
    "    scheduler.step();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\" final loss   = \" + loss.forward(model.forward(X_train), y_train).item<float>());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "loss.forward(model.forward(X_test), y_test).item<float>()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader\n",
    "\n",
    "If we wanted to be really advanced, we would use TorchSharp data sets and data loaders, which would allow us to randomize the test data set between epocs (at the end of the outer training loop). Here's how we'd do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "class SyntheticDataset : torch.utils.data.Dataset {\n",
    "\n",
    "    public SyntheticDataset(string fileName) \n",
    "    {\n",
    "        _data = torch.Tensor.load(fileName + \".x\");\n",
    "        _labels = torch.Tensor.load(fileName + \".y\");\n",
    "        if (_data.shape[0] != _labels.shape[0])\n",
    "            throw new InvalidOperationException(\"Data and labels are not of the same lengths.\");\n",
    "    }\n",
    "\n",
    "    public override Dictionary<string, torch.Tensor> GetTensor(long index)\n",
    "    {\n",
    "        var rdic = new Dictionary<string, torch.Tensor>();\n",
    "        rdic.Add(\"data\", _data[(int)index]);\n",
    "        rdic.Add(\"label\", _labels[(int)index]);\n",
    "        return rdic;\n",
    "    }\n",
    "\n",
    "    public override long Count => _data.shape[0];\n",
    "\n",
    "    private torch.Tensor _data;\n",
    "    private torch.Tensor _labels;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop gets slightly more complex with the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var training_data = new SyntheticDataset(\"train.dat\");\n",
    "var train = new torch.utils.data.DataLoader(training_data, 200, shuffle: true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "Console.WriteLine(\" initial loss = \" + loss.forward(model.forward(X_train), y_train).item<float>().ToString());\n",
    "\n",
    "for (int i = 0; i < 1000; i++) {\n",
    "\n",
    "    foreach (var data in train)\n",
    "    {\n",
    "        // Compute the loss\n",
    "        using var output = loss.forward(model.forward(data[\"data\"]), data[\"label\"]);\n",
    "\n",
    "        // Clear the gradients before doing the back-propagation\n",
    "        model.zero_grad();\n",
    "\n",
    "        // Do back-progatation, which computes all the gradients.\n",
    "        output.backward();\n",
    "\n",
    "        optimizer.step();\n",
    "    }\n",
    "    \n",
    "    scheduler.step();\n",
    "}\n",
    "\n",
    "Console.WriteLine(\" final loss   = \" + loss.forward(model.forward(X_train), y_train).item<float>());"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slower, and the convergence isn't that much better, but that will depend on the model used. You just have to try and try different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "loss.forward(model.forward(X_test), y_test).item<float>()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
